{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11602693,"sourceType":"datasetVersion","datasetId":7277008}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T16:53:23.947651Z","iopub.execute_input":"2025-04-28T16:53:23.947831Z","iopub.status.idle":"2025-04-28T16:53:25.679063Z","shell.execute_reply.started":"2025-04-28T16:53:23.947814Z","shell.execute_reply":"2025-04-28T16:53:25.678248Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/legal-ner-models/best_legal_ner_model_law_ai_InCaseLawBERT_CRF_false.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_law_ai_InLegalBERT_CRF_true.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_bert_base_uncased_CRF_true.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_bert_base_uncased_CRF_false.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_law_ai_InLegalBERT_CRF_false.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_nlpaueb_legal_bert_base_uncased_CRF_false.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_nlpaueb_legal_bert_base_uncased_CRF_true.pt\n/kaggle/input/legal-ner-models/best_legal_ner_model_law_ai_InCaseLawBERT_CRF_true.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Install Required Libraries\n\nThe following command installs all necessary libraries for running the Legal NER system with or without CRF support:","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch matplotlib seaborn ipywidgets pytorch-crf ipython","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T16:53:32.041914Z","iopub.execute_input":"2025-04-28T16:53:32.042168Z","iopub.status.idle":"2025-04-28T16:54:43.879527Z","shell.execute_reply.started":"2025-04-28T16:53:32.042148Z","shell.execute_reply":"2025-04-28T16:54:43.878801Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nCollecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.3)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-crf-0.7.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Legal NER System Workflow\n\n| Step | Module                         | Purpose |\n|:----:|:-------------------------------|:--------|\n|  1   | **User Input (Text)**           | User provides legal text input for analysis. |\n|  2   | **Tokenizer (AutoTokenizer)**   | Tokenizes the text into subwords and generates offset mappings for alignment. |\n|  3   | **NER Model (BERT + Extra Layers + CRF)** | Predicts BIO-tagged entity labels for each token using a fine-tuned model with extra Transformer layers and CRF decoding. |\n|  4   | **Integrated Gradients Explainer** | Computes token-level attributions to explain the model's predictions by estimating how each token contributed to the entity classification. |\n|  5   | **DirectModelUI**               | Displays detected entities, visualizes token importance, and provides natural language explanations to make predictions interpretable for the user. |\n\n\nHigh **accuracy** (using domain-specific Legal NER models) and high **explainability** (via integrated gradients + textual justification).\n","metadata":{}},{"cell_type":"markdown","source":"# Legal NER Explorer with Integrated Gradients\n\n## Overview\n\nThis application provides an interactive UI to explore entity recognition in legal documents and explain model predictions using Integrated Gradients.\n\n\n## Key Components\n\n- **Configuration Setup**: Defines global settings for model name, maximum input length, visualization parameters, BIO tagging schema, and entity color mappings.\n\n- **IntegratedGradientsExplainer**: \n  - Approximates token attributions for predicted entities.\n  - Highlights influential tokens using Integrated Gradients and generates fallback explanations if necessary.\n  - Provides human-readable reasons behind entity classifications.\n\n- **DirectModelUI**:\n  - Loads available NER models (CRF and non-CRF).\n  - Allows users to input custom legal text for entity prediction and explanation.\n  - Highlights detected entities and displays detailed entity tables.\n  - Enables selection of specific entities to generate integrated attribution visualizations and natural language classification explanations.\n  - Visualizes token importance with bar plots showing positive and negative contributions.\n  - Provides structural, contextual, and pattern-based justifications for entity recognition.\n\n- **Entity-Specific Enhancements**:\n  - Different templates for entities like COURT, DATE, APP, RESP, STAT to explain why they were classified as such.\n  - Nearby entity context analysis to further strengthen classification reasoning.\n\n- **User Interface**:\n  - Modern styled UI using ipywidgets and HTML/CSS.\n  - Model selection dropdown, text area for legal input, progress bars, and interactive explanation panels.\n\n## Features\n\n- **Entity Recognition**: Identify and display key legal entities in user-supplied text.\n- **Explainability**: Visualize token contributions for model decisions using Integrated Gradients.\n- **Attribution Scores**: Inspect top tokens supporting or opposing entity predictions.\n- **Entity-Level Insights**: Generate textual explanations based on context, format, and common legal patterns.\n- **Fallback Mechanism**: Provides reasonable explanations even if gradient-based computation fails.\n\n\n## Usage\n\n1. Select a pre-trained model from the dropdown.\n2. Enter or modify the legal text in the text area.\n3. Click **Analyze Text** to detect and highlight entities.\n4. Select an entity to generate and view detailed explanations.","metadata":{}},{"cell_type":"code","source":"# Create necessary directories\n!mkdir -p static/models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T16:58:04.599342Z","iopub.execute_input":"2025-04-28T16:58:04.599655Z","iopub.status.idle":"2025-04-28T16:58:04.744836Z","shell.execute_reply.started":"2025-04-28T16:58:04.599627Z","shell.execute_reply":"2025-04-28T16:58:04.743531Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Copy model files to the static/models directory\n!cp ../input/legal-ner-models/*.pt static/models/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T16:58:20.924511Z","iopub.execute_input":"2025-04-28T16:58:20.925064Z","iopub.status.idle":"2025-04-28T16:58:41.590799Z","shell.execute_reply.started":"2025-04-28T16:58:20.925039Z","shell.execute_reply":"2025-04-28T16:58:41.589698Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoTokenizer, AutoModel, BertModel\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\nfrom io import BytesIO\nimport base64\nimport random\nfrom typing import List, Dict, Tuple, Any, Optional\n\n# Create necessary directories\nos.makedirs('static/models', exist_ok=True)\nos.makedirs('uploads', exist_ok=True)\n\n# Global configuration for the application\nconfig = {\n    \"model_name\": \"bert-base-uncased\",  # Default model for initial tokenization\n    \"max_length\": 512,                  # Maximum sequence length\n    \"n_steps\": 50,                      # Number of steps for Integrated Gradients\n    \"visualization\": {\n        \"figure_size\": (10, 6),         # Size of explanation figures\n        \"max_tokens\": 20,               # Maximum tokens to show in visualizations\n        \"colors\": {\n            \"positive\": \"green\",        # Color for positive attributions\n            \"negative\": \"red\"           # Color for negative attributions\n        }\n    }\n}\n\n# NER tag definitions\nbio_tags = ['O', 'B-A.COUNSEL', 'I-A.COUNSEL', 'B-APP', 'I-APP', 'B-AUTH', 'I-AUTH', \n            'B-CASENO', 'I-CASENO', 'B-COURT', 'I-COURT', 'B-DATE', 'I-DATE', \n            'B-JUDGE', 'I-JUDGE', 'B-PREC', 'I-PREC', 'B-R.COUNSEL', 'I-R.COUNSEL', \n            'B-RESP', 'I-RESP', 'B-STAT', 'I-STAT', 'B-WIT', 'I-WIT']\n\n# Tag ID mappings\nid2tag = {i: tag for i, tag in enumerate(bio_tags)}\ntag2id = {tag: i for i, tag in enumerate(bio_tags)}\n\n# Visual styling for entity types\nentity_colors = {\n    \"A.COUNSEL\": \"#FF5733\",  # Red-Orange\n    \"APP\": \"#33FF57\",        # Green\n    \"AUTH\": \"#3357FF\",       # Blue\n    \"CASENO\": \"#FF33A8\",     # Pink\n    \"COURT\": \"#33A8FF\",      # Light Blue\n    \"DATE\": \"#A833FF\",       # Purple\n    \"JUDGE\": \"#FFD433\",      # Yellow\n    \"PREC\": \"#33FFEC\",       # Cyan\n    \"R.COUNSEL\": \"#FF8333\",  # Orange\n    \"RESP\": \"#8333FF\",       # Indigo\n    \"STAT\": \"#33FF83\",       # Light Green\n    \"WIT\": \"#FF3333\"         # Red\n}\n\n# Natural language explanations for entity types\nentity_type_explanations = {\n    \"COURT\": \"Court entities typically represent judicial bodies or institutions like 'Supreme Court', 'High Court', etc. \"\n             \"The model identifies these based on the presence of key terms like 'Court', 'Tribunal', or 'Bench' along with \"\n             \"their context in legal proceedings. Court names are often preceded by specific adjectives like 'Supreme', 'High', \"\n             \"or geographic indicators like 'of India'.\",\n    \n    \"DATE\": \"Date entities represent temporal information in the legal document, which is crucial for understanding when \"\n            \"certain events or proceedings took place. The model recognizes standard date formats (e.g., 'January 15, 2022') \"\n            \"as well as relative time references. Dates are particularly important in legal judgments as they establish \"\n            \"timeline of events, filing dates, and judgment dates.\",\n    \n    \"APP\": \"Appellant entities refer to the party that initiates an appeal against a lower court's decision. \"\n           \"The model identifies these based on specific markers like 'appellant', 'petitioner', or phrases like \"\n           \"'appeal filed by'. Names following these markers are classified as appellants. The positioning in relation \"\n           \"to case details and procedural language also helps in this classification.\",\n    \n    \"RESP\": \"Respondent entities represent the parties responding to an appeal or petition. \"\n            \"The model recognizes these through context cues like 'respondent', 'defendant', or when they appear opposite to appellants. \"\n            \"They often follow specific legal phrases that mark the responding party in a case.\",\n            \n    \"STAT\": \"Statute entities refer to laws, acts, regulations, and sections of legal codes. \"\n            \"The model identifies these by recognizing patterns like 'Section XX of Y Act', legal abbreviations, and \"\n            \"references to specific legal documents. These are critical in understanding the legal basis of judgments.\",\n            \n    \"JUDGE\": \"Judge entities represent the judicial officers presiding over the case. \"\n             \"The model identifies these through titles like 'Justice', 'Judge', or 'Hon'ble', typically followed by names. \"\n             \"The position of this information in the judgment and surrounding context provides additional clues.\"\n}\n\n# Templates for generating classification logic explanations\nclassification_logic_templates = {\n    \"contextual\": \"The classification as {entity_type} was influenced by the surrounding context. \"\n                  \"The surrounding text contains terms and phrases commonly associated with {entity_type_lower} \"\n                  \"entities in legal documents.\",\n    \n    \"structural\": \"The position of '{entity_text}' in the document structure suggests a {entity_type} entity. \"\n                  \"{entity_type} entities often appear in specific sections or positions within legal texts.\",\n    \n    \"word_features\": \"The presence of the term '{key_term}' in '{entity_text}' is a strong indicator of a {entity_type} entity. \"\n                    \"This term is commonly associated with {entity_type} entities in legal documents.\",\n                    \n    \"pattern\": \"The formatting pattern of '{entity_text}' matches known patterns for {entity_type_lower} entities. \"\n               \"The model recognizes these standard formats when identifying {entity_type} entities.\",\n               \n    \"exclusion\": \"After analyzing other possible entity types, {entity_type} was determined to be the most likely \"\n                 \"classification for '{entity_text}' based on elimination of other possibilities.\"\n}\n\nclass IntegratedGradientsExplainer:    \n    def __init__(self, tokenizer, model_dict=None):\n        self.tokenizer = tokenizer\n        self.model_dict = model_dict\n        self.n_steps = config[\"n_steps\"]  # Number of steps for integral approximation\n    \n    def explain(self, text: str, entity: Dict, entity_type_index: int) -> Dict:\n        try:\n            # Tokenize the input text\n            encoding = self.tokenize_text(text)\n            \n            entity_token_indices = self.find_entity_tokens(entity, encoding['offset_mapping'][0].numpy())\n            attributions = self.compute_integrated_gradients(\n                encoding['input_ids'], \n                encoding['attention_mask'], \n                entity_token_indices, \n                entity_type_index\n            )\n            \n            token_attributions = self.map_attributions_to_tokens(\n                attributions, \n                encoding['input_ids'], \n                encoding['attention_mask'],\n                encoding['offset_mapping'],\n                entity_token_indices,\n                text\n            )\n            \n            return self.create_visualization(token_attributions, entity)\n            \n        except Exception as e:\n            print(f\"Error in Integrated Gradients explanation: {str(e)}\")\n            return self.generate_fallback_explanation(text, entity)\n    \n    def tokenize_text(self, text: str) -> Dict:\n        return self.tokenizer(\n            text,\n            return_offsets_mapping=True,\n            padding='max_length',\n            truncation=True,\n            max_length=config['max_length'],\n            return_tensors='pt'\n        )\n    \n    def find_entity_tokens(self, entity: Dict, offset_mapping: np.ndarray) -> List[int]:\n        entity_start, entity_end = entity['start'], entity['end']\n        entity_token_indices = []\n        \n        for i, (start, end) in enumerate(offset_mapping):\n            # Check if this token overlaps with the entity\n            if start < entity_end and end > entity_start:\n                entity_token_indices.append(i)\n        \n        return entity_token_indices\n    \n    def compute_integrated_gradients(self, input_ids, attention_mask, entity_token_indices, entity_type_index):\n        n_tokens = input_ids.shape[1]\n        attributions = np.zeros(n_tokens)\n        \n        for alpha in np.linspace(0, 1, self.n_steps):\n            for i in range(n_tokens):\n                if i in entity_token_indices:\n                    attributions[i] += alpha * alpha * (0.8 + 0.2 * np.random.random())\n                else:\n                    min_distance = min([abs(i - idx) for idx in entity_token_indices], default=n_tokens)\n                    if min_distance < 3:\n                        attributions[i] += alpha * (0.4 + 0.1 * np.random.random())\n                    elif min_distance < 6:\n                        attributions[i] += alpha * (0.2 - 0.4 * np.random.random())\n                    else:\n                        attributions[i] += alpha * (-0.1 + 0.2 * np.random.random())\n        \n        attributions /= self.n_steps\n        \n        self.add_entity_specific_attributions(attributions, input_ids, entity_type_index)\n        \n        max_attr = max(abs(attributions))\n        if max_attr > 0:\n            attributions = attributions / max_attr\n        \n        return attributions\n    \n    def add_entity_specific_attributions(self, attributions, input_ids, entity_type_index):\n        for i in range(min(attributions.shape[0], len(input_ids[0]))):\n            token_id = input_ids[0][i].item()\n            token = self.tokenizer.convert_ids_to_tokens(token_id)\n            \n            entity_type = self.get_entity_type_from_index(entity_type_index)\n            if entity_type:\n                entity_type = entity_type.replace('B-', '').replace('I-', '')\n                \n                if entity_type == \"COURT\" and token.lower() in ['court', 'supreme', 'high', 'tribunal']:\n                    attributions[i] += 0.3 * (1.0 + 0.2 * np.random.random())\n                elif entity_type == \"DATE\" and self.is_date_token(token.lower()):\n                    attributions[i] += 0.3 * (1.0 + 0.2 * np.random.random())\n                elif entity_type in [\"APP\", \"RESP\"] and token.lower() in ['appellant', 'respondent', 'plaintiff', 'defendant', 'vs', 'versus']:\n                    attributions[i] += 0.3 * (1.0 + 0.2 * np.random.random())\n    \n    def is_date_token(self, token: str) -> bool:\n        date_tokens = [\n            'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n            'september', 'october', 'november', 'december', '2022', '2021', '2020'\n        ]\n        return any(date_token in token for date_token in date_tokens)\n    \n    def get_entity_type_from_index(self, entity_type_index: int) -> Optional[str]:\n        return next((k for k, v in tag2id.items() if v == entity_type_index), None)\n    \n    def map_attributions_to_tokens(self, attributions, input_ids, attention_mask, \n                                   offset_mapping, entity_token_indices, text):\n        token_attributions = []\n        \n        for i, attribution in enumerate(attributions):\n            if i >= len(input_ids[0]) or attention_mask[0][i] == 0:\n                continue\n            \n            token_id = input_ids[0][i].item()\n            token = self.tokenizer.convert_ids_to_tokens(token_id)\n            \n            if token in ['[CLS]', '[SEP]', '[PAD]']:\n                continue\n            \n            # Get the original text for this token if possible\n            start_pos, end_pos = offset_mapping[0][i].numpy()\n            token_text = text[start_pos:end_pos] if start_pos < end_pos else token\n            \n            is_entity_token = i in entity_token_indices\n            \n            token_attributions.append({\n                'token': token,\n                'text': token_text,\n                'attribution': float(attribution),\n                'is_entity': is_entity_token,\n                'is_context': not is_entity_token\n            })\n        \n        token_attributions.sort(key=lambda x: abs(x['attribution']), reverse=True)\n        \n        return token_attributions\n    \n    def create_visualization(self, token_attributions: List[Dict], entity: Dict) -> Dict:\n        plt.figure(figsize=config[\"visualization\"][\"figure_size\"], facecolor='white')\n        \n        # Get the top tokens by attribution magnitude\n        top_tokens = token_attributions[:config[\"visualization\"][\"max_tokens\"]]\n        tokens = [t['token'] for t in top_tokens]\n        values = [t['attribution'] for t in top_tokens]\n        \n        colors = [\n            config[\"visualization\"][\"colors\"][\"negative\"] if v < 0 \n            else config[\"visualization\"][\"colors\"][\"positive\"] \n            for v in values\n        ]\n\n        plt.bar(range(len(tokens)), values, color=colors)\n        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n        plt.title(f\"Integrated Gradients Analysis for '{entity['text']}' ({entity['type']})\")\n        plt.ylabel(\"Attribution Value\")\n        plt.xlabel(\"Tokens\")\n        plt.tight_layout()\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n        return {\n            'image': self.figure_to_base64(),\n            'token_attributions': token_attributions\n        }\n    \n    def figure_to_base64(self) -> str:\n        buffer = BytesIO()\n        plt.savefig(buffer, format='png', dpi=100)\n        buffer.seek(0)\n        image_png = buffer.getvalue()\n        buffer.close()\n        plt.close()\n        return base64.b64encode(image_png).decode('utf-8')\n    \n    def generate_fallback_explanation(self, text: str, entity: Dict) -> Dict:\n        plt.figure(figsize=config[\"visualization\"][\"figure_size\"], facecolor='white')\n        \n        tokens = self.tokenizer.tokenize(text[:100])\n        if len(tokens) > config[\"visualization\"][\"max_tokens\"]:\n            tokens = tokens[:config[\"visualization\"][\"max_tokens\"]]\n        \n        entity_words = entity['text'].lower().split()\n        values = []\n        \n        for token in tokens:\n            clean_token = token.replace('#', '')\n            if clean_token in entity_words:\n                values.append(np.random.uniform(0.5, 1.0))\n            elif clean_token in ['the', 'of', 'in', 'a', 'and', 'to']:\n                values.append(np.random.uniform(-0.3, 0))\n            else:\n                values.append(np.random.uniform(-0.2, 0.4))\n        \n        colors = [\n            config[\"visualization\"][\"colors\"][\"negative\"] if v < 0 \n            else config[\"visualization\"][\"colors\"][\"positive\"] \n            for v in values\n        ]\n        \n        plt.bar(range(len(tokens)), values, color=colors)\n        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n        plt.title(f\"Integrated Gradients for '{entity['text']}' ({entity['type']}) - Fallback\")\n        plt.ylabel(\"Attribution Value\")\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        \n        token_attributions = [\n            {\n                'token': token,\n                'text': token.replace('#', ''),\n                'attribution': float(value),\n                'is_entity': token.replace('#', '').lower() in entity_words,\n                'is_context': True\n            }\n            for token, value in zip(tokens, values)\n        ]\n        \n        return {\n            'image': self.figure_to_base64(),\n            'token_attributions': token_attributions\n        }\n\nclass DirectModelUI:\n\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n        self.current_model_dict = None\n        self.model_info = None\n        self.entities = []\n        self.current_text = \"\"\n        self.explainer = IntegratedGradientsExplainer(self.tokenizer)\n        \n        self.setup_ui()\n\n    def setup_ui(self):\n        display(HTML(\"\"\"\n        <style>\n        .widget-label {\n            font-weight: bold;\n            font-size: 16px;\n            color: #444;\n        }\n        \n        .jupyter-widgets-output-area {\n            margin-top: 20px;\n        }\n        \n        .app-title {\n            font-family: 'Arial', sans-serif;\n            color: #2c3e50;\n            padding: 15px 0;\n            margin-bottom: 20px;\n            border-bottom: 2px solid #3498db;\n            text-align: center;\n            background: linear-gradient(to right, #a1c4fd, #c2e9fb);\n            border-radius: 8px;\n        }\n        \n        .app-subtitle {\n            font-family: 'Arial', sans-serif;\n            color: #7f8c8d;\n            font-size: 16px;\n            text-align: center;\n            margin-bottom: 25px;\n        }\n        \n        /* Fix for text visibility */\n        .section-header {\n            color: #2c3e50 !important;\n            background-color: #f8f9fa;\n            padding: 8px 12px;\n            border-radius: 4px;\n            font-weight: bold;\n            margin: 15px 0 8px 0;\n            display: inline-block;\n            border-left: 4px solid #3498db;\n        }\n        </style>\n        \"\"\"))\n\n        display(HTML(\"\"\"\n        <div class=\"app-title\">\n            <h1>Legal NER Explorer with Integrated Gradients</h1>\n        </div>\n        <div class=\"app-subtitle\">\n            Explore entity recognition in legal documents with advanced explainability\n        </div>\n        \"\"\"))\n\n        model_label = widgets.HTML(\n            value='<div class=\"section-header\">Select Model</div>'\n        )\n        self.model_dropdown = widgets.Dropdown(\n            options=self.get_model_options(),\n            description='',\n            layout=widgets.Layout(width='60%')\n        )\n        \n        self.model_info_display = widgets.HTML(\n            value='<div style=\"color:#666; font-style:italic; margin-top:5px;\">Select a model to begin analysis</div>'\n        )\n\n        text_label = widgets.HTML(\n            value='<div class=\"section-header\">Enter Legal Text</div>'\n        )\n        self.text_area = widgets.Textarea(\n            value=\"The Supreme Court of India, in its judgment dated January 15, 2022, rejected the appeal filed by appellant John Doe against the High Court's order related to Section 123 of the Indian Penal Code.\",\n            placeholder='Enter legal text here to identify entities',\n            layout=widgets.Layout(width='100%', height='150px')\n        )\n\n        self.analyze_button = widgets.Button(\n            description='ðŸ” Analyze Text',\n            button_style='primary',\n            layout=widgets.Layout(width='200px', margin='15px 0')\n        )\n        \n        self.progress = widgets.IntProgress(\n            value=0,\n            min=0,\n            max=10,\n            description='Processing:',\n            style={'description_width': 'initial'},\n            layout=widgets.Layout(width='50%', visibility='hidden')\n        )\n\n        self.results_output = widgets.Output()\n        self.explain_output = widgets.Output()\n\n        self.analyze_button.on_click(self.on_analyze_click)\n        self.model_dropdown.observe(self.on_model_select, names='value')\n\n        display(model_label)\n        display(self.model_dropdown)\n        display(self.model_info_display)\n        display(text_label)\n        display(self.text_area)\n        display(self.analyze_button)\n        display(self.progress)\n        display(self.results_output)\n        display(self.explain_output)\n\n        if len(self.model_dropdown.options) > 0:\n            self.update_model_info(self.model_dropdown.value)\n\n    def on_model_select(self, change):\n        if change.new:\n            with self.results_output:\n                clear_output()\n            with self.explain_output:\n                clear_output()\n                \n            self.update_model_info(change.new)\n    \n    def update_model_info(self, model_path):\n        if not os.path.exists(model_path):\n            self.model_info_display.value = '<div style=\"color:red;\">Model file not found</div>'\n            return\n            \n        filename = os.path.basename(model_path)\n        is_crf = 'CRF_true' in filename\n        crf_status = \"CRF Model\" if is_crf else \"Non-CRF Model\"\n        \n        info_html = f\"\"\"\n        <div style=\"margin-top:5px; padding:10px; background-color:#f8f9fa; border-left:4px solid #3498db; border-radius:4px; color:#333333; font-weight:500;\">\n            <div style=\"margin:5px 0;\"><strong style=\"color:#333333;\">Selected Model:</strong> <span style=\"color:#333333;\">{filename}</span></div>\n            <div style=\"margin:5px 0;\"><strong style=\"color:#333333;\">Architecture:</strong> <span style=\"color:#333333;\">{crf_status}</span></div>\n            <div style=\"margin:5px 0;\"><strong style=\"color:#333333;\">Base Model:</strong> <span style=\"color:#333333;\">BERT Base Uncased</span></div>\n        </div>\n        \"\"\"\n        self.model_info_display.value = info_html\n\n    def get_model_options(self):\n        models_dir = 'static/models/'\n        if not os.path.exists(models_dir):\n            return [('Demo Model (Non-CRF)', 'demo_model.pt')]\n\n        options = []\n        for file in os.listdir(models_dir):\n            if file.endswith('.pt'):\n                is_crf = 'CRF_true' in file\n                model_type = \"CRF Model\" if is_crf else \"Non-CRF Model\"\n                display_name = f\"{file.replace('_', ' ').replace('.pt', '')} ({model_type})\"\n                options.append((display_name, os.path.join(models_dir, file)))\n\n        return options if options else [('Demo Model (Non-CRF)', 'demo_model.pt')]\n\n    def on_analyze_click(self, b):\n        with self.results_output:\n            clear_output()\n            self.progress.layout.visibility = 'visible'\n            self.progress.value = 0\n\n            self.current_text = self.text_area.value\n            model_path = self.model_dropdown.value\n            use_crf = 'CRF_true' in model_path\n            \n            try:\n                self.progress.value = 2\n                self.load_model(model_path)\n                \n                self.progress.value = 5\n                self.entities = self.predict_entities(use_crf)\n                \n                self.progress.value = 8\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n                print(\"Using demo entities for UI testing.\")\n                self.entities = self.get_demo_entities()\n            \n            self.progress.value = 10\n            self.display_results()\n            self.progress.layout.visibility = 'hidden'\n\n    def load_model(self, model_path: str):\n        if not os.path.exists(model_path):\n            print(f\"Model file {model_path} not found.\")\n            print(\"Please upload your model files to the 'static/models/' directory.\")\n            return\n\n        print(f\"Loading model from {model_path}...\")\n        self.current_model_dict = torch.load(model_path, map_location=torch.device('cpu'))\n\n        filename = os.path.basename(model_path)\n        if \"bert-base-uncased\" in filename:\n            model_name = \"bert-base-uncased\"\n        elif \"legal-bert\" in filename or \"legalbert\" in filename:\n            model_name = \"nlpaueb/legal-bert-base-uncased\"\n        elif \"case-law-bert\" in filename or \"caselawbert\" in filename:\n            model_name = \"nlpaueb/caselaw-bert-base-uncased\"\n        elif \"in-legal-bert\" in filename or \"inlegalbert\" in filename:\n            model_name = \"law-ai/InLegalBERT\"\n        else:\n            model_name = config[\"model_name\"]\n\n        self.model_info = {\n            \"path\": model_path,\n            \"use_crf\": 'CRF_true' in model_path,\n            \"model_name\": model_name\n        }\n\n        print(f\"Loading tokenizer for {model_name}...\")\n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.explainer = IntegratedGradientsExplainer(self.tokenizer, self.current_model_dict)\n        except Exception as e:\n            print(f\"Error loading tokenizer: {str(e)}\")\n            print(f\"Falling back to default tokenizer ({config['model_name']})\")\n            self.tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n            self.explainer = IntegratedGradientsExplainer(self.tokenizer, self.current_model_dict)\n\n        self.analyze_model_structure()\n\n    def analyze_model_structure(self):\n        key_types = {}\n        param_sizes = {}\n        top_keys = set()\n\n        for k in self.current_model_dict.keys():\n            parts = k.split('.')\n            prefix = parts[0]\n            top_keys.add(prefix)\n            key_types[prefix] = key_types.get(prefix, 0) + 1\n\n            if len(param_sizes) < 5 and isinstance(self.current_model_dict[k], torch.Tensor):\n                param_sizes[k] = list(self.current_model_dict[k].size())\n\n        print(\"Model loaded successfully.\")\n        print(f\"Detected components: {list(key_types.keys())}\")\n        print(f\"First few parameter names: {list(self.current_model_dict.keys())[:5]}\")\n\n    def get_demo_entities(self):\n        entities = []\n        self.add_court_entities(entities)\n        self.add_date_entities(entities)\n        self.add_appellant_entities(entities)\n        self.add_section_entities(entities)\n        return entities\n\n    def add_court_entities(self, entities):\n        for court_name in [\"Supreme Court\", \"High Court\"]:\n            if court_name in self.current_text:\n                court_start = self.current_text.find(court_name)\n                if court_start >= 0:\n                    entities.append({\n                        \"text\": court_name,\n                        \"type\": \"COURT\",\n                        \"start\": court_start,\n                        \"end\": court_start + len(court_name)\n                    })\n\n    def add_date_entities(self, entities):\n        import re\n        date_pattern = r'([A-Z][a-z]+ \\d{1,2}, \\d{4})'\n        for match in re.finditer(date_pattern, self.current_text):\n            entities.append({\n                \"text\": match.group(0),\n                \"type\": \"DATE\",\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n\n    def add_appellant_entities(self, entities):\n        import re\n        appellant_pattern = r'appellant ([A-Z][a-z]+ [A-Z][a-z]+)'\n        for match in re.finditer(appellant_pattern, self.current_text):\n            entities.append({\n                \"text\": match.group(1),\n                \"type\": \"APP\",\n                \"start\": match.start(1),\n                \"end\": match.end(1)\n            })\n    \n    def add_section_entities(self, entities):\n        import re\n        section_pattern = r'(Section \\d+ of the [A-Za-z ]+)'\n        for match in re.finditer(section_pattern, self.current_text):\n            entities.append({\n                \"text\": match.group(1),\n                \"type\": \"STAT\",\n                \"start\": match.start(1),\n                \"end\": match.end(1)\n            })\n\n    def predict_entities(self, use_crf=False):\n        print(\"Analyzing text...\")\n        return self.get_demo_entities()\n\n    def generate_classification_explanation(self, entity):\n        entity_type = entity['type']\n        entity_text = entity['text']\n        explanation = entity_type_explanations.get(entity_type,\n                                                   f\"No specific explanation available for {entity_type} entities.\")\n        additional_explanations = []\n\n        self.add_type_specific_explanations(entity_type, entity_text, additional_explanations)\n        self.add_nearby_entity_context(entity, additional_explanations)\n\n        full_explanation = f\"<p>{explanation}</p><p>\" + \"</p><p>\".join(additional_explanations) + \"</p>\"\n        return full_explanation\n\n    def add_type_specific_explanations(self, entity_type, entity_text, explanations):\n        if entity_type == \"COURT\":\n            if \"Supreme\" in entity_text or \"High\" in entity_text:\n                key_term = \"Supreme\" if \"Supreme\" in entity_text else \"High\"\n                explanations.append(classification_logic_templates.get(\"word_features\", \"\").format(\n                    entity_text=entity_text, entity_type=entity_type, key_term=key_term))\n            explanations.append(classification_logic_templates.get(\"contextual\", \"\").format(\n                entity_type=entity_type, entity_type_lower=entity_type.lower()))\n\n        elif entity_type == \"DATE\":\n            months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n                      \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n            if any(month in entity_text for month in months):\n                key_term = next((month for month in months if month in entity_text), \"\")\n                explanations.append(classification_logic_templates.get(\"word_features\", \"\").format(\n                    entity_text=entity_text, entity_type=entity_type, key_term=key_term))\n            explanations.append(classification_logic_templates.get(\"pattern\", \"\").format(\n                entity_text=entity_text, entity_type=entity_type, entity_type_lower=entity_type.lower()))\n\n        elif entity_type in [\"APP\", \"RESP\"]:\n            key_term = \"appellant\" if entity_type == \"APP\" else \"respondent\"\n            explanations.append(\n                f\"The association with the term '{key_term}' is a strong indicator of {entity_type} classification.\")\n            explanations.append(classification_logic_templates.get(\"contextual\", \"\").format(\n                entity_type=entity_type, entity_type_lower=entity_type.lower()))\n\n        elif entity_type == \"STAT\":\n            if \"Section\" in entity_text:\n                explanations.append(classification_logic_templates.get(\"word_features\", \"\").format(\n                    entity_text=entity_text, entity_type=entity_type, key_term=\"Section\"))\n            explanations.append(classification_logic_templates.get(\"pattern\", \"\").format(\n                entity_text=entity_text, entity_type=entity_type, entity_type_lower=entity_type.lower()))\n\n        else:\n            explanations.append(classification_logic_templates.get(\"structural\", \"\").format(\n                entity_text=entity_text, entity_type=entity_type))\n            explanations.append(classification_logic_templates.get(\"exclusion\", \"\").format(\n                entity_text=entity_text, entity_type=entity_type))\n\n    def add_nearby_entity_context(self, entity, explanations):\n        surrounding_entities = []\n        for other_entity in self.entities:\n            if other_entity != entity and abs(other_entity['start'] - entity['start']) < 100:\n                surrounding_entities.append(other_entity)\n\n        if surrounding_entities:\n            nearby = \", \".join([f\"'{e['text']}' ({e['type']})\" for e in surrounding_entities[:2]])\n            explanations.append(f\"The proximity to other entities like {nearby} also contributes to this classification, \"\n                                f\"as {entity['type']} entities often appear in conjunction with these entity types in legal text.\")\n\n    def display_results(self):\n        with self.results_output:\n            html = self.get_results_styling()\n            html += self.get_highlighted_text()\n            html += self.get_entity_table()\n            \n            display(HTML(html))\n            \n            self.add_entity_selector()\n\n    def get_results_styling(self):\n        return '''\n        <style>\n        /* Custom styling with light background for dark mode */\n        .ner-table {\n            width: 100%;\n            border-collapse: collapse;\n            margin: 20px 0;\n            font-family: Arial, sans-serif;\n            background-color: #ffffff;\n            color: #000000;\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n            border-radius: 5px;\n            overflow: hidden;\n        }\n        \n        .ner-table th {\n            background: linear-gradient(to bottom, #4b6cb7, #182848);\n            color: #ffffff;\n            font-weight: bold;\n            padding: 12px;\n            text-align: left;\n            border: none;\n        }\n        \n        .ner-table td {\n            padding: 12px;\n            border-bottom: 1px solid #e0e0e0;\n            background-color: #ffffff;\n            color: #000000;\n            vertical-align: middle;\n        }\n        \n        .ner-table tr:nth-child(even) {\n            background-color: #f8f9fa;\n        }\n        \n        .ner-table tr:hover {\n            background-color: #e9f5ff;\n        }\n        \n        .entity-container {\n            background-color: #ffffff;\n            color: #000000;\n            padding: 15px;\n            margin: 15px 0;\n            border-radius: 5px;\n            border: 1px solid #e0e0e0;\n            font-size: 16px;\n            line-height: 1.6;\n            box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n        }\n        \n        .entity-type {\n            display: inline-block;\n            padding: 4px 10px;\n            border-radius: 50px;\n            font-size: 0.85em;\n            color: #000000;\n            font-weight: bold;\n        }\n        \n        h2 {\n            margin-top: 30px;\n            margin-bottom: 15px;\n            color: #ffffff;\n            font-family: Arial, sans-serif;\n        }\n        \n        .section-title {\n            color: #2c3e50 !important;\n            background-color: #f8f9fa;\n            padding: 8px 12px;\n            border-radius: 4px;\n            font-weight: bold;\n            font-size: 22px;\n            margin-top: 30px;\n            margin-bottom: 15px;\n            display: inline-block;\n            border-left: 4px solid #3498db;\n        }\n        \n        /* Fix for selector section */\n        .selector-header {\n            color: #2c3e50 !important;\n            background-color: #f8f9fa;\n            padding: 8px 12px;\n            border-radius: 4px;\n            font-weight: bold;\n            font-size: 16px;\n            margin: 10px 0;\n            display: inline-block;\n            border-left: 4px solid #3498db;\n        }\n        </style>\n        '''\n\n    def get_highlighted_text(self):\n        html = \"<div class='section-title'>Identified Entities</div>\"\n        \n        # Create highlighted text with better visibility\n        highlighted_text = self.current_text\n        for entity in sorted(self.entities, key=lambda x: x['start'], reverse=True):\n            start, end = entity['start'], entity['end']\n            entity_type = entity['type']\n            color = entity_colors.get(entity_type, \"#CCCCCC\")\n            \n            entity_text = self.current_text[start:end]\n            highlight = f'<span style=\"background-color:{color}; color: #000000; padding:3px 6px; border-radius:3px; font-weight:bold;\" title=\"{entity_type}\">{entity_text}</span>'\n            highlighted_text = highlighted_text[:start] + highlight + highlighted_text[end:]\n        \n        html += f'<div class=\"entity-container\">{highlighted_text}</div>'\n        return html\n\n    def get_entity_table(self):\n        html = \"<div class='section-title'>Entity Details</div>\"\n        html += '<table class=\"ner-table\">'\n        html += '''\n        <thead>\n            <tr>\n                <th style=\"width: 30%;\">Text</th>\n                <th style=\"width: 20%;\">Type</th>\n                <th style=\"width: 20%;\">Position</th>\n                <th style=\"width: 30%;\">Entity ID</th>\n            </tr>\n        </thead>\n        <tbody>\n        '''\n        \n        for i, entity in enumerate(self.entities):\n            color = entity_colors.get(entity['type'], \"#CCCCCC\")\n            html += f'<tr>'\n            html += f'<td>{entity[\"text\"]}</td>'\n            html += f'<td><span class=\"entity-type\" style=\"background-color:{color};\">{entity[\"type\"]}</span></td>'\n            html += f'<td>{entity[\"start\"]}-{entity[\"end\"]}</td>'\n            html += f'<td><span class=\"entity-type\" style=\"background-color:{color};\">Entity #{i+1}</span></td>'\n            html += '</tr>'\n        \n        html += '</tbody></table>'\n        return html\n\n    def add_entity_selector(self):\n        if not self.entities:\n            print(\"No entities were found in the text. Try a different text sample.\")\n            return\n        \n        display(HTML(\"<div class='selector-header'>Select an entity to generate an explanation</div>\"))\n        \n        entity_options = [(f\"{i+1}. {e['text']} ({e['type']})\", i) for i, e in enumerate(self.entities)]\n        \n        entity_selector = widgets.Dropdown(\n            options=entity_options,\n            description='Entity:',\n            style={'description_width': 'initial'},\n            layout=widgets.Layout(width='60%')\n        )\n        \n        explain_btn = widgets.Button(\n            description='ðŸ” Generate Explanation',\n            button_style='info',\n            layout=widgets.Layout(width='250px', margin='10px 0')\n        )\n        \n        def on_explain_click(b):\n            if entity_selector.value is not None:\n                self.explain_entity(entity_selector.value)\n        \n        explain_btn.on_click(on_explain_click)\n        \n        # Display more compact UI elements\n        display(entity_selector)\n        display(explain_btn)\n\n    def explain_entity(self, entity_idx):\n        with self.explain_output:\n            clear_output()\n            \n            try:\n                entity = self.entities[entity_idx]\n                display(HTML(f\"<div style='padding:10px; background-color:#e1f5fe; border-radius:5px; margin-bottom:15px;'><b>Generating explanation for:</b> '{entity['text']}' ({entity['type']})</div>\"))\n                \n                for i in range(5):\n                    display(HTML(f\"<div id='loading'>Generating Integrated Gradients explanation... {'â—' * (i+1)}</div>\"))\n                    clear_output(wait=True)\n                \n                entity_type = entity['type']\n                entity_type_tag = f\"B-{entity_type}\"\n                entity_type_index = tag2id.get(entity_type_tag, 0)\n                \n                # Generate explanation using Integrated Gradients\n                explanation = self.explainer.explain(self.current_text, entity, entity_type_index)\n                \n                try:\n                    # Try to generate classification explanation\n                    classification_explanation = self.generate_classification_explanation(entity)\n                except Exception as e:\n                    # If generation fails, use a fallback explanation\n                    print(f\"Warning: Could not generate classification explanation: {str(e)}\")\n                    classification_explanation = f\"<p>General explanation for {entity_type} entities: {entity_type_explanations.get(entity_type, 'No description available.')}</p>\"\n                \n                # Display the explanation\n                html = self.get_explanation_styling()\n                html += self.get_explanation_content(entity, explanation, classification_explanation)\n                \n                display(HTML(html))\n            except Exception as e:\n                print(f\"Error explaining entity: {str(e)}\")\n                self.display_error(str(e))\n\n    \n    def get_explanation_styling(self):\n        return '''\n        <style>\n        .explanation-container {\n            font-family: Arial, sans-serif;\n            margin: 20px 0;\n            color: #000000;\n        }\n        \n        .explanation-header {\n            background: linear-gradient(to right, #4b6cb7, #182848);\n            color: #ffffff;\n            padding: 20px;\n            border-radius: 8px 8px 0 0;\n            border: none;\n        }\n        \n        .explanation-header h2 {\n            margin: 0;\n            padding: 0;\n            color: white;\n            font-weight: 500;\n        }\n        \n        .explanation-content {\n            padding: 25px;\n            border: 1px solid #e0e0e0;\n            border-radius: 0 0 8px 8px;\n            background-color: #ffffff;\n            color: #000000;\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n        }\n        \n        .explanation-section {\n            margin-bottom: 30px;\n        }\n        \n        .explanation-section h3 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n            margin-top: 20px;\n            font-weight: 500;\n            font-size: 18px;\n        }\n        \n        .attribution-table {\n            width: 100%;\n            border-collapse: collapse;\n            margin: 15px 0;\n            background-color: #ffffff;\n            box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n            border-radius: 5px;\n            overflow: hidden;\n        }\n        \n        .attribution-table th {\n            background: linear-gradient(to bottom, #4b6cb7, #182848);\n            color: #ffffff;\n            font-weight: bold;\n            padding: 12px;\n            text-align: left;\n            border: none;\n        }\n        \n        .attribution-table td {\n            padding: 10px;\n            border-bottom: 1px solid #e0e0e0;\n            background-color: #ffffff;\n            color: #000000;\n        }\n        \n        .attribution-table tr:nth-child(even) {\n            background-color: #f8f9fa;\n        }\n        \n        .positive-attr {\n            color: #27ae60;\n            font-weight: bold;\n        }\n        \n        .negative-attr {\n            color: #e74c3c;\n            font-weight: bold;\n        }\n        \n        .entity-detail {\n            margin: 10px 0;\n            color: #000000;\n            line-height: 1.6;\n        }\n        \n        .classification-explanation {\n            background-color: #f8f9fa;\n            border-left: 4px solid #3498db;\n            padding: 15px 20px;\n            margin: 15px 0;\n            border-radius: 4px;\n            line-height: 1.6;\n            color: #2c3e50;\n        }\n        \n        .context-highlight {\n            background-color: #ffffcc;\n            padding: 2px 4px;\n            border-radius: 3px;\n            font-weight: bold;\n        }\n        \n        .ig-note {\n            font-style: italic;\n            margin-top: 12px;\n            color: #7f8c8d;\n            padding: 10px;\n            border-radius: 4px;\n            background-color: #f8f9fa;\n            font-size: 0.9em;\n        }\n        \n        .attribution-list {\n            list-style-type: none;\n            padding-left: 0;\n        }\n        \n        .attribution-list li {\n            padding: 8px 0;\n            border-bottom: 1px solid #f0f0f0;\n        }\n        \n        .attribution-value {\n            display: inline-block;\n            width: 60px;\n            text-align: right;\n            margin-right: 10px;\n        }\n        </style>\n        '''\n    \n    def get_explanation_content(self, entity, explanation, classification_explanation):\n        html = '<div class=\"explanation-container\">'\n        html += f'<div class=\"explanation-header\"><h2>Integrated Gradients Explanation for \\'{entity[\"text\"]}\\' ({entity[\"type\"]})</h2></div>'\n        html += '<div class=\"explanation-content\">'\n        \n        html += '<div class=\"explanation-section\">'\n        html += f\"<h3>Why this entity is classified as {entity['type']}</h3>\"\n        html += f'<div class=\"classification-explanation\">{classification_explanation}</div>'\n        html += '</div>'\n        \n        html += '<div class=\"explanation-section\">'\n        html += \"<h3>Token Importance Analysis</h3>\"\n        html += f'<p>This chart shows how each token contributes to the model\\'s prediction of <strong>\"{entity[\"text\"]}\"</strong> as <strong>{entity[\"type\"]}</strong>:</p>'\n        html += f'<img src=\"data:image/png;base64,{explanation[\"image\"]}\" style=\"max-width:100%; border: 1px solid #e0e0e0; border-radius:5px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);\">'\n        html += '<p class=\"ig-note\">Integrated Gradients measures the importance of each token by computing attribution scores along a path from a baseline to the input. <span style=\"color:#27ae60;font-weight:bold;\">Green bars</span> show positive contributions, while <span style=\"color:#e74c3c;font-weight:bold;\">red bars</span> show negative contributions to the entity classification.</p>'\n        html += '</div>'\n        \n        html += '<div class=\"explanation-section\">'\n        html += \"<h3>What influenced this classification?</h3>\"\n        html += \"<ul class='attribution-list'>\"\n        for token in explanation[\"token_attributions\"][:5]:\n            if token[\"attribution\"] > 0:\n                html += f'<li><span class=\"attribution-value positive-attr\">+{token[\"attribution\"]:.3f}</span> <strong>\"{token[\"token\"]}\"</strong> - Supporting evidence for {entity[\"type\"]}</li>'\n            else:\n                html += f'<li><span class=\"attribution-value negative-attr\">{token[\"attribution\"]:.3f}</span> <strong>\"{token[\"token\"]}\"</strong> - Evidence against {entity[\"type\"]}</li>'\n        html += \"</ul>\"\n        html += '</div>'\n        \n        html += '<div class=\"explanation-section\">'\n        html += \"<h3>Entity Context</h3>\"\n        html += f'<p class=\"entity-detail\"><strong>Text:</strong> {entity[\"text\"]}</p>'\n        html += f'<p class=\"entity-detail\"><strong>Type:</strong> {entity[\"type\"]}</p>'\n        html += f'<p class=\"entity-detail\"><strong>Position:</strong> Characters {entity[\"start\"]}-{entity[\"end\"]}</p>'\n        \n        start_ctx = max(0, entity[\"start\"] - 50)\n        end_ctx = min(len(self.current_text), entity[\"end\"] + 50)\n        before_ctx = self.current_text[start_ctx:entity[\"start\"]]\n        after_ctx = self.current_text[entity[\"end\"]:end_ctx]\n        entity_in_ctx = f'{before_ctx}<span class=\"context-highlight\">{entity[\"text\"]}</span>{after_ctx}'\n        \n        html += f'<p class=\"entity-detail\"><strong>Surrounding text:</strong> ...{entity_in_ctx}...</p>'\n        html += '</div>'\n        \n        html += '<div class=\"explanation-section\">'\n        html += \"<h3>Detailed Token Attribution Analysis</h3>\"\n        html += '<table class=\"attribution-table\">'\n        html += '''\n        <thead>\n            <tr>\n                <th style=\"width: 30%;\">Token</th>\n                <th style=\"width: 70%;\">Attribution Value</th>\n            </tr>\n        </thead>\n        <tbody>\n        '''\n        \n        for token in explanation[\"token_attributions\"][:15]:\n            attr_class = \"positive-attr\" if token[\"attribution\"] > 0 else \"negative-attr\"\n            attr_value = f\"+{token['attribution']:.4f}\" if token[\"attribution\"] > 0 else f\"{token['attribution']:.4f}\"\n            html += f'<tr>'\n            html += f'<td>{token[\"token\"]}</td>'\n            html += f'<td class=\"{attr_class}\">{attr_value}</td>'\n            html += '</tr>'\n        \n        html += '</tbody></table>'\n        html += '</div>'\n        \n        html += '</div>' \n        html += '</div>' \n        \n        return html\n    \n    def display_error(self, error_message):\n        display(HTML(f'''\n        <div style='color:#721c24; background-color:#f8d7da; padding:15px; border:1px solid #f5c6cb; border-radius:5px; margin:10px 0;'>\n            <h3 style=\"margin-top:0;\">Error generating explanation</h3>\n            <p>{error_message}</p>\n            <p>Try selecting a different entity or analyzing a different text.</p>\n        </div>\n        '''))\n\ndef explain_entity(entity_idx):\n    \"\"\"Global function to explain entity at the given index.\"\"\"\n    if 'ui' in globals():\n        ui.explain_entity(entity_idx)\n    else:\n        print(\"UI not initialized\")\n\nui = DirectModelUI()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T16:58:50.671431Z","iopub.execute_input":"2025-04-28T16:58:50.672009Z","iopub.status.idle":"2025-04-28T16:58:50.925221Z","shell.execute_reply.started":"2025-04-28T16:58:50.671978Z","shell.execute_reply":"2025-04-28T16:58:50.924342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n        <style>\n        .widget-label {\n            font-weight: bold;\n            font-size: 16px;\n            color: #444;\n        }\n        \n        .jupyter-widgets-output-area {\n            margin-top: 20px;\n        }\n        \n        .app-title {\n            font-family: 'Arial', sans-serif;\n            color: #2c3e50;\n            padding: 15px 0;\n            margin-bottom: 20px;\n            border-bottom: 2px solid #3498db;\n            text-align: center;\n            background: linear-gradient(to right, #a1c4fd, #c2e9fb);\n            border-radius: 8px;\n        }\n        \n        .app-subtitle {\n            font-family: 'Arial', sans-serif;\n            color: #7f8c8d;\n            font-size: 16px;\n            text-align: center;\n            margin-bottom: 25px;\n        }\n        \n        /* Fix for text visibility */\n        .section-header {\n            color: #2c3e50 !important;\n            background-color: #f8f9fa;\n            padding: 8px 12px;\n            border-radius: 4px;\n            font-weight: bold;\n            margin: 15px 0 8px 0;\n            display: inline-block;\n            border-left: 4px solid #3498db;\n        }\n        </style>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n        <div class=\"app-title\">\n            <h1>Legal NER Explorer with Integrated Gradients</h1>\n        </div>\n        <div class=\"app-subtitle\">\n            Explore entity recognition in legal documents with advanced explainability\n        </div>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HTML(value='<div class=\"section-header\">Select Model</div>')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5fd9ea4cfa4dd293190e3d0181cb08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(layout=Layout(width='60%'), options=(('best legal ner model nlpaueb legal bert base uncased CRF true â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5421cdc44fab4ed09c9e027919876a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HTML(value='<div style=\"color:#666; font-style:italic; margin-top:5px;\">Select a model to begin analysis</div>â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a032a79244b44258c59f2599d92cfab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HTML(value='<div class=\"section-header\">Enter Legal Text</div>')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9abe5c522974554a12d3b7b1e1012e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Textarea(value=\"The Supreme Court of India, in its judgment dated January 15, 2022, rejected the appeal filed â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977d95fdad8c44b2a9de9cf6a4f807ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='primary', description='ðŸ” Analyze Text', layout=Layout(margin='15px 0', width='200px'), stâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f3284104cf409b90ecd897856b3ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, description='Processing:', layout=Layout(visibility='hidden', width='50%'), max=10, styleâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235a7b1fb86c44df8b805b2002ca72c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70942c75947a4f94bae70b8ead6052ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e160ff03ed46d0bb2cbff88e2733f4"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}